

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>model.denoising.model package &mdash; GWAI  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="model.denoising.mpu package" href="model.denoising.mpu.html" />
    <link rel="prev" title="model.denoising.fused_kernels package" href="model.denoising.fused_kernels.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> GWAI
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="about_gwai.html">About GWAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">src</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="data.html">data package</a></li>
<li class="toctree-l2"><a class="reference internal" href="eval.html">eval package</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="model.html">model package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="model.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="model.classify.html">model.classify package</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="model.denoising.html">model.denoising package</a></li>
<li class="toctree-l4"><a class="reference internal" href="model.detection.html">model.detection package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="model.html#module-model">Module contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">utils package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="citations.html">Citations</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GWAI</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="modules.html">src</a> &raquo;</li>
        
          <li><a href="model.html">model package</a> &raquo;</li>
        
          <li><a href="model.denoising.html">model.denoising package</a> &raquo;</li>
        
      <li>model.denoising.model package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/model.denoising.model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="model-denoising-model-package">
<h1>model.denoising.model package<a class="headerlink" href="#model-denoising-model-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-model.denoising.model.distributed">
<span id="model-denoising-model-distributed-module"></span><h2>model.denoising.model.distributed module<a class="headerlink" href="#module-model.denoising.model.distributed" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallel">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.distributed.</span></span><span class="sig-name descname"><span class="pre">DistributedDataParallel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate_allreduce_grads_in_fp32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_contiguous_buffers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.distributed.DistributedDataParallelBase" title="model.denoising.model.distributed.DistributedDataParallelBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.distributed.DistributedDataParallelBase</span></code></a></p>
<p>DDP with contiguous buffers options to storre and accumulate gradients.
This class:</p>
<blockquote>
<div><ul class="simple">
<li><p>has the potential to reduce memory fragmentation.</p></li>
<li><p>provides the option to do the gradient accumulation
in a type other than the params type (for example fp32)</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – input model.</p></li>
<li><p><strong>accumulate_allreduce_grads_in_fp32</strong> – if true do the gradient accumulation
and the gradient all-reduce all in in float32. If this option is
true, we require <cite>use_contiguous_buffers</cite> to be true too.</p></li>
<li><p><strong>use_contiguous_buffers</strong> – if true, use a contiguous buffer to store the
gradients.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallel.allreduce_gradients">
<span class="sig-name descname"><span class="pre">allreduce_gradients</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallel.allreduce_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduce gradients across data parallel ranks.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallel.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallel.zero_grad_buffer">
<span class="sig-name descname"><span class="pre">zero_grad_buffer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallel.zero_grad_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the grad buffer data to zero. Needs to be called at the
begining of each iteration.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallelBase">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.distributed.</span></span><span class="sig-name descname"><span class="pre">DistributedDataParallelBase</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallelBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Abstract class for DDP.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallelBase.allreduce_gradients">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">allreduce_gradients</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallelBase.allreduce_gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallelBase.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallelBase.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallelBase.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallelBase.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#model.denoising.model.distributed.DistributedDataParallelBase.state_dict" title="model.denoising.model.distributed.DistributedDataParallelBase.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#model.denoising.model.distributed.DistributedDataParallelBase.state_dict" title="model.denoising.model.distributed.DistributedDataParallelBase.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#model.denoising.model.distributed.DistributedDataParallelBase.state_dict" title="model.denoising.model.distributed.DistributedDataParallelBase.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#model.denoising.model.distributed.DistributedDataParallelBase.state_dict" title="model.denoising.model.distributed.DistributedDataParallelBase.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#model.denoising.model.distributed.DistributedDataParallelBase.load_state_dict" title="model.denoising.model.distributed.DistributedDataParallelBase.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallelBase.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallelBase.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallelBase.state_dict_for_save_checkpoint">
<span class="sig-name descname"><span class="pre">state_dict_for_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallelBase.state_dict_for_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this function to override the state dict for
saving checkpoints.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.distributed.DistributedDataParallelBase.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.distributed.DistributedDataParallelBase.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.distributed.MemoryBuffer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.distributed.</span></span><span class="sig-name descname"><span class="pre">MemoryBuffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">numel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.MemoryBuffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.MemoryBuffer.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_index</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.MemoryBuffer.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tensor with the input <cite>shape</cite> as a view into the
1-D data starting at <cite>start_index</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.distributed.MemoryBuffer.zero">
<span class="sig-name descname"><span class="pre">zero</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.distributed.MemoryBuffer.zero" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the buffer to zero.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-model.denoising.model.enums">
<span id="model-denoising-model-enums-module"></span><h2>model.denoising.model.enums module<a class="headerlink" href="#module-model.denoising.model.enums" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.enums.AttnMaskType">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.enums.</span></span><span class="sig-name descname"><span class="pre">AttnMaskType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.enums.AttnMaskType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.enums.AttnMaskType.causal">
<span class="sig-name descname"><span class="pre">causal</span></span><em class="property"> <span class="pre">=</span> <span class="pre">2</span></em><a class="headerlink" href="#model.denoising.model.enums.AttnMaskType.causal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.enums.AttnMaskType.padding">
<span class="sig-name descname"><span class="pre">padding</span></span><em class="property"> <span class="pre">=</span> <span class="pre">1</span></em><a class="headerlink" href="#model.denoising.model.enums.AttnMaskType.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.enums.AttnType">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.enums.</span></span><span class="sig-name descname"><span class="pre">AttnType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.enums.AttnType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.enums.AttnType.cross_attn">
<span class="sig-name descname"><span class="pre">cross_attn</span></span><em class="property"> <span class="pre">=</span> <span class="pre">2</span></em><a class="headerlink" href="#model.denoising.model.enums.AttnType.cross_attn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.enums.AttnType.self_attn">
<span class="sig-name descname"><span class="pre">self_attn</span></span><em class="property"> <span class="pre">=</span> <span class="pre">1</span></em><a class="headerlink" href="#model.denoising.model.enums.AttnType.self_attn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.enums.AttnType.value_conv">
<span class="sig-name descname"><span class="pre">value_conv</span></span><em class="property"> <span class="pre">=</span> <span class="pre">3</span></em><a class="headerlink" href="#model.denoising.model.enums.AttnType.value_conv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.enums.LayerType">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.enums.</span></span><span class="sig-name descname"><span class="pre">LayerType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.enums.LayerType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.enums.LayerType.decoder">
<span class="sig-name descname"><span class="pre">decoder</span></span><em class="property"> <span class="pre">=</span> <span class="pre">2</span></em><a class="headerlink" href="#model.denoising.model.enums.LayerType.decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.enums.LayerType.encoder">
<span class="sig-name descname"><span class="pre">encoder</span></span><em class="property"> <span class="pre">=</span> <span class="pre">1</span></em><a class="headerlink" href="#model.denoising.model.enums.LayerType.encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-model.denoising.model.fused_bias_gelu">
<span id="model-denoising-model-fused-bias-gelu-module"></span><h2>model.denoising.model.fused_bias_gelu module<a class="headerlink" href="#module-model.denoising.model.fused_bias_gelu" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.fused_bias_gelu.GeLUFunction">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.fused_bias_gelu.</span></span><span class="sig-name descname"><span class="pre">GeLUFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_bias_gelu.GeLUFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_bias_gelu.GeLUFunction.backward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_bias_gelu.GeLUFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#model.denoising.model.fused_bias_gelu.GeLUFunction.forward" title="model.denoising.model.fused_bias_gelu.GeLUFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#model.denoising.model.fused_bias_gelu.GeLUFunction.forward" title="model.denoising.model.fused_bias_gelu.GeLUFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#model.denoising.model.fused_bias_gelu.GeLUFunction.backward" title="model.denoising.model.fused_bias_gelu.GeLUFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#model.denoising.model.fused_bias_gelu.GeLUFunction.forward" title="model.denoising.model.fused_bias_gelu.GeLUFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_bias_gelu.GeLUFunction.forward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_bias_gelu.GeLUFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.fused_bias_gelu.bias_gelu_impl">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.fused_bias_gelu.</span></span><span class="sig-name descname"><span class="pre">bias_gelu_impl</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_bias_gelu.bias_gelu_impl" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-model.denoising.model.fused_layer_norm">
<span id="model-denoising-model-fused-layer-norm-module"></span><h2>model.denoising.model.fused_layer_norm module<a class="headerlink" href="#module-model.denoising.model.fused_layer_norm" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>This code is copied fron NVIDIA apex:</dt><dd><p><a class="reference external" href="https://github.com/NVIDIA/apex">https://github.com/NVIDIA/apex</a></p>
</dd>
</dl>
<p>with some changes.</p>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.fused_layer_norm.</span></span><span class="sig-name descname"><span class="pre">FusedLayerNormAffineFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.backward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward" title="model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward" title="model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.backward" title="model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward" title="model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_layer_norm.FusedLayerNormAffineFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.fused_layer_norm.MixedFusedLayerNorm">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.fused_layer_norm.</span></span><span class="sig-name descname"><span class="pre">MixedFusedLayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_layer_norm.MixedFusedLayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_layer_norm.MixedFusedLayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_layer_norm.MixedFusedLayerNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_layer_norm.MixedFusedLayerNorm.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_layer_norm.MixedFusedLayerNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.fused_layer_norm.MixedFusedLayerNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.fused_layer_norm.MixedFusedLayerNorm.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-model.denoising.model.fused_softmax">
<span id="model-denoising-model-fused-softmax-module"></span><h2>model.denoising.model.fused_softmax module<a class="headerlink" href="#module-model.denoising.model.fused_softmax" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.FusedScaleMaskSoftmax">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.fused_softmax.</span></span><span class="sig-name descname"><span class="pre">FusedScaleMaskSoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_in_fp16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_in_bf16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled_masked_softmax_fusion</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax_in_fp32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.FusedScaleMaskSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>fused operation: scaling + mask + softmax
:param input_in_fp16: flag to indicate if input in fp16 data format.
:param attn_mask_type: attention mask type (pad or causal)
:param mask_func: mask function to be applied.
:param softmax_in_fp32: if true, softmax in performed at fp32 precision.
:param scale: scaling factor used in input tensor scaling.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.FusedScaleMaskSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.FusedScaleMaskSoftmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.FusedScaleMaskSoftmax.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.fused_softmax.FusedScaleMaskSoftmax.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.ScaledMaskedSoftmax">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.fused_softmax.</span></span><span class="sig-name descname"><span class="pre">ScaledMaskedSoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.ScaledMaskedSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>Fused operation which performs following three operations in sequence
1. Scale the tensor.
2. Apply the mask.
3. Perform softmax.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.ScaledMaskedSoftmax.backward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grads</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.ScaledMaskedSoftmax.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward" title="model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward" title="model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledMaskedSoftmax.backward" title="model.denoising.model.fused_softmax.ScaledMaskedSoftmax.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward" title="model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.ScaledMaskedSoftmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.fused_softmax.</span></span><span class="sig-name descname"><span class="pre">ScaledUpperTriangMaskedSoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>Fused operation which performs following three operations in sequence
1. Scale the tensor.
2. Apply upper triangular mask (typically used in gpt models).
3. Perform softmax.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.backward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grads</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward" title="model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward" title="model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.backward" title="model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward" title="model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.fused_softmax.ScaledUpperTriangMaskedSoftmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-model.denoising.model.module">
<span id="model-denoising-model-module-module"></span><h2>model.denoising.model.module module<a class="headerlink" href="#module-model.denoising.model.module" title="Permalink to this headline">¶</a></h2>
<p>Megatron Module</p>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.module.Float16Module">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.module.</span></span><span class="sig-name descname"><span class="pre">Float16Module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.Float16Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.module.Float16Module.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.Float16Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.module.Float16Module.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.Float16Module.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#model.denoising.model.module.Float16Module.state_dict" title="model.denoising.model.module.Float16Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#model.denoising.model.module.Float16Module.state_dict" title="model.denoising.model.module.Float16Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#model.denoising.model.module.Float16Module.state_dict" title="model.denoising.model.module.Float16Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#model.denoising.model.module.Float16Module.state_dict" title="model.denoising.model.module.Float16Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#model.denoising.model.module.Float16Module.load_state_dict" title="model.denoising.model.module.Float16Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.module.Float16Module.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.Float16Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.module.Float16Module.state_dict_for_save_checkpoint">
<span class="sig-name descname"><span class="pre">state_dict_for_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.Float16Module.state_dict_for_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this function to override the state dict for
saving checkpoints.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.module.Float16Module.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.module.Float16Module.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.module.MegatronModule">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.module.</span></span><span class="sig-name descname"><span class="pre">MegatronModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">share_word_embeddings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.MegatronModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Megatron specific extensions of torch Module with support
for pipelining.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.module.MegatronModule.initialize_word_embeddings">
<span class="sig-name descname"><span class="pre">initialize_word_embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_method_normal</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.MegatronModule.initialize_word_embeddings" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.module.MegatronModule.state_dict_for_save_checkpoint">
<span class="sig-name descname"><span class="pre">state_dict_for_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.MegatronModule.state_dict_for_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this function to override the state dict for
saving checkpoints.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.module.MegatronModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.module.MegatronModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.module.MegatronModule.word_embeddings_weight">
<span class="sig-name descname"><span class="pre">word_embeddings_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.MegatronModule.word_embeddings_weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.module.conversion_helper">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.module.</span></span><span class="sig-name descname"><span class="pre">conversion_helper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conversion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.conversion_helper" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply conversion to val. Recursively apply conversion if <cite>val</cite>
#is a nested tuple/list structure.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.module.float16_to_fp32">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.module.</span></span><span class="sig-name descname"><span class="pre">float16_to_fp32</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.float16_to_fp32" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert fp16/bf16 <cite>val</cite> to fp32</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.module.fp32_to_float16">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.module.</span></span><span class="sig-name descname"><span class="pre">fp32_to_float16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float16_convertor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.fp32_to_float16" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert fp32 <cite>val</cite> to fp16/bf16</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.module.param_is_not_shared">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.module.</span></span><span class="sig-name descname"><span class="pre">param_is_not_shared</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.module.param_is_not_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-model.denoising.model.transformer">
<span id="model-denoising-model-transformer-module"></span><h2>model.denoising.model.transformer module<a class="headerlink" href="#module-model.denoising.model.transformer" title="Permalink to this headline">¶</a></h2>
<p>Transformer.</p>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelAttention">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.transformer.</span></span><span class="sig-name descname"><span class="pre">ParallelAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layer_init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AttnType.self_attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AttnMaskType.padding</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>Parallel self-attention layer abstract class.</p>
<p>Self-attention layer takes input with size [b, s, h]
and returns output of the same size.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_key_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_atten_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.transformer.ParallelAttention.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelMLP">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.transformer.</span></span><span class="sig-name descname"><span class="pre">ParallelMLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layer_init_method</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelMLP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>MLP.</p>
<p>MLP will take the input with h hidden state, project it to 4*h
hidden dimension, perform nonlinear transformation, and project the
state back into h hidden dimension. At the end, dropout is also
applied.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelMLP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelMLP.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelMLP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.transformer.ParallelMLP.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelTransformer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.transformer.</span></span><span class="sig-name descname"><span class="pre">ParallelTransformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layer_init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">LayerType.encoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_mask_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AttnMaskType.padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_atten_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelTransformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>Transformer class.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelTransformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_key_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_dec_attn_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelTransformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelTransformer.set_input_tensor">
<span class="sig-name descname"><span class="pre">set_input_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelTransformer.set_input_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Set input tensor to be used instead of forward()’s input.</p>
<p>When doing pipeline parallelism the input from the previous
stage comes from communication, not from the input, so the
model’s forward_step_func won’t have it. This function is thus
used by internal code to bypass the input provided by the
forward_step_func</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelTransformer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.transformer.ParallelTransformer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelTransformerLayer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.transformer.</span></span><span class="sig-name descname"><span class="pre">ParallelTransformerLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layer_init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">LayerType.encoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_mask_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AttnMaskType.padding</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelTransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>A single transformer layer.</p>
<p>Transformer layer takes input with size [b, s, h] and returns an
output of the same size.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelTransformerLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_dec_attn_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_key_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_atten_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.ParallelTransformerLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.transformer.ParallelTransformerLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.transformer.ParallelTransformerLayer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.transformer.SwiGLU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.transformer.</span></span><span class="sig-name descname"><span class="pre">SwiGLU</span></span><a class="headerlink" href="#model.denoising.model.transformer.SwiGLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.transformer.SwiGLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.SwiGLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.transformer.SwiGLU.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.transformer.SwiGLU.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.transformer.bias_dropout_add">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.transformer.</span></span><span class="sig-name descname"><span class="pre">bias_dropout_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">residual</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#model.denoising.model.transformer.bias_dropout_add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.transformer.get_bias_dropout_add">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.transformer.</span></span><span class="sig-name descname"><span class="pre">get_bias_dropout_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.transformer.get_bias_dropout_add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-model.denoising.model.utils">
<span id="model-denoising-model-utils-module"></span><h2>model.denoising.model.utils module<a class="headerlink" href="#module-model.denoising.model.utils" title="Permalink to this headline">¶</a></h2>
<p>Utilities for models.</p>
<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.utils.attention_mask_func">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.utils.</span></span><span class="sig-name descname"><span class="pre">attention_mask_func</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attention_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.utils.attention_mask_func" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.utils.get_linear_layer">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.utils.</span></span><span class="sig-name descname"><span class="pre">get_linear_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">columns</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.utils.get_linear_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple linear layer with weight initialization.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.utils.init_method_normal">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.utils.</span></span><span class="sig-name descname"><span class="pre">init_method_normal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sigma</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.utils.init_method_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Init method based on N(0, sigma).</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.utils.openai_gelu">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.utils.</span></span><span class="sig-name descname"><span class="pre">openai_gelu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.utils.openai_gelu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.utils.scaled_init_method_normal">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.utils.</span></span><span class="sig-name descname"><span class="pre">scaled_init_method_normal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sigma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.utils.scaled_init_method_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Init method based on N(0, sigma/sqrt(2*num_layers).</p>
</dd></dl>

</div>
<div class="section" id="module-model.denoising.model.waveform_model">
<span id="model-denoising-model-waveform-model-module"></span><h2>model.denoising.model.waveform_model module<a class="headerlink" href="#module-model.denoising.model.waveform_model" title="Permalink to this headline">¶</a></h2>
<p>Transformer based language model.</p>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Embedding">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveform_model.</span></span><span class="sig-name descname"><span class="pre">Embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sequence_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dropout_prob</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_tokentypes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>Language model embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> – hidden size</p></li>
<li><p><strong>vocab_size</strong> – vocabulary size</p></li>
<li><p><strong>max_sequence_length</strong> – maximum size of sequence. This
is used for positional embedding</p></li>
<li><p><strong>embedding_dropout_prob</strong> – dropout probability for embeddings</p></li>
<li><p><strong>init_method</strong> – weight initialization method</p></li>
<li><p><strong>num_tokentypes</strong> – size of the token-type embeddings. 0 value
will ignore this embedding</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Embedding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokentype_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.Embedding.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Embedding.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.Embedding.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Customized load.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Embedding.state_dict_for_save_checkpoint">
<span class="sig-name descname"><span class="pre">state_dict_for_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.Embedding.state_dict_for_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>For easy load.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Embedding.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.waveform_model.Embedding.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Pooler">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveform_model.</span></span><span class="sig-name descname"><span class="pre">Pooler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.Pooler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>Pooler layer.</p>
<p>Pool hidden states of a specific token (for example start of the
sequence) and add a linear transformation followed by a tanh.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> – hidden size</p></li>
<li><p><strong>init_method</strong> – weight initialization method for the linear layer.
bias is set to zero.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Pooler.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.Pooler.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.Pooler.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.waveform_model.Pooler.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.TransformerWaveformModel">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveform_model.</span></span><span class="sig-name descname"><span class="pre">TransformerWaveformModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_layer_init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_attn_mask_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_tokentypes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_decoder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_attn_mask_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AttnMaskType.causal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_pooler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_atten_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.TransformerWaveformModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>Transformer language model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transformer_hparams</strong> – transformer hyperparameters</p></li>
<li><p><strong>vocab_size</strong> – vocabulary size</p></li>
<li><p><strong>max_sequence_length</strong> – maximum size of sequence. This
is used for positional embedding</p></li>
<li><p><strong>embedding_dropout_prob</strong> – dropout probability for embeddings</p></li>
<li><p><strong>num_tokentypes</strong> – size of the token-type embeddings. 0 value
will ignore this embedding</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.TransformerWaveformModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enc_input_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_position_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_attn_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dec_input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dec_position_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dec_attn_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_dec_attn_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokentype_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_key_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling_sequence_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_enc_hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.TransformerWaveformModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.TransformerWaveformModel.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.TransformerWaveformModel.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Customized load.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.TransformerWaveformModel.set_input_tensor">
<span class="sig-name descname"><span class="pre">set_input_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.TransformerWaveformModel.set_input_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>See megatron.model.transformer.set_input_tensor()</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.TransformerWaveformModel.state_dict_for_save_checkpoint">
<span class="sig-name descname"><span class="pre">state_dict_for_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.TransformerWaveformModel.state_dict_for_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>For easy load.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.TransformerWaveformModel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.waveform_model.TransformerWaveformModel.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.get_waveform_model">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveform_model.</span></span><span class="sig-name descname"><span class="pre">get_waveform_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_tokentypes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_pooler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_attn_mask_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled_init_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_decoder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_attn_mask_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AttnMaskType.causal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_atten_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.get_waveform_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Build language model and return along with the key to save.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.waveform_model.parallel_gw_logits">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveform_model.</span></span><span class="sig-name descname"><span class="pre">parallel_gw_logits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_embeddings_weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveform_model.parallel_gw_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>LM logits using word embedding weights.</p>
</dd></dl>

</div>
<div class="section" id="module-model.denoising.model.waveformer_model">
<span id="model-denoising-model-waveformer-model-module"></span><h2>model.denoising.model.waveformer_model module<a class="headerlink" href="#module-model.denoising.model.waveformer_model" title="Permalink to this headline">¶</a></h2>
<p>WaveFormer model.</p>
<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.GWHead">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveformer_model.</span></span><span class="sig-name descname"><span class="pre">GWHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layernorm_epsilon</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_output</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.GWHead" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>Masked GW head for WaveFormer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> – hidden size</p></li>
<li><p><strong>init_method</strong> – init method for weight initialization</p></li>
<li><p><strong>layernorm_epsilon</strong> – tolerance for layer norm divisions</p></li>
<li><p><strong>parallel_output</strong> – whether output logits being distributed or not.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.GWHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_embeddings_weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.GWHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.GWHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.waveformer_model.GWHead.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.WaveFormerModel">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveformer_model.</span></span><span class="sig-name descname"><span class="pre">WaveFormerModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_tokentypes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_binary_head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_process</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_atten_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.WaveFormerModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#model.denoising.model.module.MegatronModule" title="model.denoising.model.module.MegatronModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">model.denoising.model.module.MegatronModule</span></code></a></p>
<p>Bert Language model.</p>
<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.WaveFormerModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bert_model_input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokentype_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gw_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.WaveFormerModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.WaveFormerModel.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.WaveFormerModel.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Customized load.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.WaveFormerModel.set_input_tensor">
<span class="sig-name descname"><span class="pre">set_input_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.WaveFormerModel.set_input_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>See megatron.model.transformer.set_input_tensor()</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.WaveFormerModel.state_dict_for_save_checkpoint">
<span class="sig-name descname"><span class="pre">state_dict_for_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.WaveFormerModel.state_dict_for_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>For easy load when model is combined with other heads,
add an extra key.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.WaveFormerModel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#model.denoising.model.waveformer_model.WaveFormerModel.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.bert_position_ids">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveformer_model.</span></span><span class="sig-name descname"><span class="pre">bert_position_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.bert_position_ids" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.gw_extended_attention_mask">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveformer_model.</span></span><span class="sig-name descname"><span class="pre">gw_extended_attention_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.gw_extended_attention_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="model.denoising.model.waveformer_model.post_waveform_model_processing">
<span class="sig-prename descclassname"><span class="pre">model.denoising.model.waveformer_model.</span></span><span class="sig-name descname"><span class="pre">post_waveform_model_processing</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gw_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooled_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gw_head</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary_head</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logit_weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_atten_value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model.denoising.model.waveformer_model.post_waveform_model_processing" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-model.denoising.model">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-model.denoising.model" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="model.denoising.mpu.html" class="btn btn-neutral float-right" title="model.denoising.mpu package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="model.denoising.fused_kernels.html" class="btn btn-neutral float-left" title="model.denoising.fused_kernels package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2024, Yue Zhou, Tianyu Zhao.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>